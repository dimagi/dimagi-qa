name: Metrics Collector

on:
  workflow_run:
    workflows:
      - "HQ Smoke Tests"
      - "Python Request API"
      - "CO BHA Smoke Tests"
      - "Lookup table Tests"
      - "Case Search Tests"
      - "Case Search Split Screen Tests"
      - "Data Dictionary Tests"
      - "Elastic Search Tests"
      - "Export Tests"
      - "Find Data by ID Tests"
      - "Formplayer Tests"
      - "Multi Select Tests"
      - "Priority Escape Defects Tests"
      - "Power BI Tests"
    types: [completed]

  workflow_dispatch:
    inputs:
      run_id:
        description: "GitHub Actions run_id to ingest (from any branch)"
        required: false
        type: string

  # Daily digest (10:00 AM IST = 04:30 UTC)
  schedule:
    - cron: "30 4 * * *"

permissions:
  contents: write
  actions: read

jobs:
  collect:
    # âœ… Only run after a workflow_run is COMPLETE (not per job), or manual ingest
    if: >
      (github.event_name == 'workflow_run'
        && github.event.action == 'completed'
        && github.event.workflow_run.status == 'completed'
        && github.event.workflow_run.conclusion != 'cancelled')
      || (github.event_name == 'workflow_dispatch' && inputs.run_id != '')
    runs-on: ubuntu-latest

    steps:
      - name: Checkout metrics branch
        uses: actions/checkout@v4
        with:
          ref: metrics
          fetch-depth: 0

      - name: Determine run id
        id: runid
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "RID=${{ inputs.run_id }}" >> "$GITHUB_OUTPUT"
          else
            echo "RID=${{ github.event.workflow_run.id }}" >> "$GITHUB_OUTPUT"
          fi

      - name: Download run-summary artifacts (supports matrix + legacy)
        id: dl
        uses: actions/github-script@v7
        with:
          result-encoding: json
          script: |
            const runId = Number("${{ steps.runid.outputs.RID }}");
            const owner = context.repo.owner;
            const repo = context.repo.repo;

            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
              owner, repo, run_id: runId
            });

            const all = artifacts.data.artifacts || [];
            const targets = all.filter(a => a.name === "run-summary" || a.name.startsWith("run-summary-"));

            if (!targets.length) {
              core.notice(
                `No run-summary artifacts found for run_id=${runId}. ` +
                `Available: ${all.map(a => a.name).join(", ") || "(none)"}`
              );
              core.setOutput("FOUND", "false");
              return { found: false, count: 0, names: [] };
            }

            const fs = require("fs");
            for (const a of targets) {
              const dl = await github.rest.actions.downloadArtifact({
                owner, repo, artifact_id: a.id, archive_format: "zip"
              });
              const safeName = a.name.replace(/[^a-zA-Z0-9._-]/g, "_");
              const out = `run-summary__${safeName}.zip`;
              fs.writeFileSync(out, Buffer.from(dl.data));
              core.info(`Downloaded ${a.name} -> ${out}`);
            }

            core.setOutput("FOUND", "true");
            return { found: true, count: targets.length, names: targets.map(t => t.name) };

      - name: Unzip run summaries
        if: ${{ steps.dl.outputs.FOUND == 'true' }}
        run: |
          sudo apt-get update && sudo apt-get install -y unzip
          mkdir -p incoming
          for z in run-summary__run-summary*.zip; do
            d="incoming/${z%.zip}"
            mkdir -p "$d"
            echo "Unzipping $z -> $d"
            unzip -o "$z" -d "$d"
          done
          echo "Found run_summary.json files:"
          find incoming -type f -name "run_summary.json" -print

      - name: Append to dataset
        if: ${{ steps.dl.outputs.FOUND == 'true' }}
        run: |
          mkdir -p metrics
          found_any=0
          while IFS= read -r f; do
            found_any=1
            cat "$f" >> metrics/runs.jsonl
            echo "" >> metrics/runs.jsonl
          done < <(find incoming -type f -name "run_summary.json")

          if [ "$found_any" -eq 0 ]; then
            echo "No run_summary.json found after unzip. Skipping dataset update."
            exit 0
          fi

      - name: Build site
        if: ${{ steps.dl.outputs.FOUND == 'true' }}
        run: |
          mkdir -p site/data
          cp metrics/runs.jsonl site/data/runs.jsonl

          if [ -d "dashboard" ]; then
            echo "Dashboard folder found. Building site."
            cp -r dashboard/* site/
          else
            echo "No dashboard folder present. Skipping site build."
          fi
      

      - name: Commit and push
        if: ${{ steps.dl.outputs.FOUND == 'true' }}
        run: |
          git config user.name "metrics-bot"
          git config user.email "metrics-bot@users.noreply.github.com"
          git add metrics/runs.jsonl site/
          git commit -m "Metrics update from run ${{ steps.runid.outputs.RID }}" || echo "No changes"
          git push origin metrics

  daily_digest:
    if: ${{ github.event_name == 'schedule' }}
    runs-on: ubuntu-latest

    steps:
      - name: Checkout metrics branch
        uses: actions/checkout@v4
        with:
          ref: metrics
          fetch-depth: 0

      - name: Build digest message (last 24h)
        id: digest
        run: |
          python - <<'PY'
          import json, os
          from datetime import datetime, timedelta, timezone
          from collections import defaultdict

          path = "metrics/runs.jsonl"
          now = datetime.now(timezone.utc)
          window_start = now - timedelta(hours=24)

          runs = []
          if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
              for line in f:
                line = line.strip()
                if not line:
                  continue
                try:
                  d = json.loads(line)
                  ts = d.get("timestamp_utc")
                  if not ts:
                    continue
                  t = datetime.fromisoformat(ts.replace("Z", "+00:00"))
                  if t >= window_start:
                    runs.append(d)
                except Exception:
                  continue

          if not runs:
            msg = "ðŸ“Š *Daily Automation Dashboard* (last 24h)\nNo runs recorded in the last 24 hours."
            print("HAS_DATA=true")
            print("MSG<<EOF")
            print(msg)
            print("EOF")
            raise SystemExit(0)

          total_runs = len(runs)
          pass_runs = sum(1 for r in runs if r.get("status") == "pass")
          fail_runs = total_runs - pass_runs

          by_env = defaultdict(lambda: {"runs":0, "pass":0, "fail":0, "reruns":0})
          by_suite_fail = defaultdict(int)

          for r in runs:
            env = r.get("env","unknown")
            suite = r.get("suite","unknown")
            status = r.get("status","unknown")
            by_env[env]["runs"] += 1
            if status == "pass":
              by_env[env]["pass"] += 1
            else:
              by_env[env]["fail"] += 1
              by_suite_fail[suite] += 1
            by_env[env]["reruns"] += int(r.get("rerun_count") or 0)

          top_fail = sorted(by_suite_fail.items(), key=lambda x: x[1], reverse=True)[:5]
          top_fail_txt = ", ".join([f"{k} ({v})" for k,v in top_fail]) if top_fail else "None"

          env_lines = []
          for env, s in sorted(by_env.items(), key=lambda x: x[0]):
            env_lines.append(f"â€¢ *{env}*: runs {s['runs']} | âœ… {s['pass']} | âŒ {s['fail']} | reruns {s['reruns']}")

          dashboard_url = os.environ.get("DASHBOARD_URL","").strip()
          dash_line = f"\nðŸ“ˆ Dashboard: {dashboard_url}" if dashboard_url else ""

          msg = (
            f"ðŸ“Š *Daily Automation Dashboard* (last 24h)\n"
            f"Total runs: *{total_runs}* | âœ… *{pass_runs}* | âŒ *{fail_runs}*\n\n"
            f"*By environment:*\n" + "\n".join(env_lines) + "\n\n"
            f"*Top failing suites:* {top_fail_txt}"
            + dash_line
          )

          print("HAS_DATA=true")
          print("MSG<<EOF")
          print(msg)
          print("EOF")
          PY
        env:
          DASHBOARD_URL: ${{ vars.AUTOMATION_DASHBOARD_URL }}

      - name: Post/Update digest message in Slack (single pinned-style message)
        env:
          SLACK_QA_BOT_TOKEN: ${{ secrets.SLACK_QA_BOT_TOKEN }}
          SLACK_CHANNEL_ID: ${{ secrets.SLACK_CHANNEL_ID_DASHBOARD }}
          DIGEST_MESSAGE: ${{ steps.digest.outputs.MSG }}
        run: |
          pip install --quiet slack_sdk
          python - <<'PY'
          import json, os
          from pathlib import Path
          from slack_sdk import WebClient

          token = os.environ["SLACK_QA_BOT_TOKEN"]
          channel = os.environ["SLACK_CHANNEL_ID"]
          msg = os.environ["DIGEST_MESSAGE"]

          state_path = Path("metrics/slack_dashboard_message.json")
          state_path.parent.mkdir(parents=True, exist_ok=True)

          client = WebClient(token=token)

          ts = None
          if state_path.exists():
            try:
              ts = json.loads(state_path.read_text(encoding="utf-8")).get("ts")
            except Exception:
              ts = None

          if ts:
            client.chat_update(channel=channel, ts=ts, text=msg)
          else:
            resp = client.chat_postMessage(channel=channel, text=msg)
            ts = resp["ts"]
            state_path.write_text(json.dumps({"ts": ts}, indent=2), encoding="utf-8")

          print(f"âœ… Dashboard message updated (ts={ts})")
          PY

      - name: Commit Slack message state (ts)
        run: |
          git config user.name "metrics-bot"
          git config user.email "metrics-bot@users.noreply.github.com"
          git add metrics/slack_dashboard_message.json
          git commit -m "Update dashboard Slack message state" || echo "No changes"
          git push origin metrics
