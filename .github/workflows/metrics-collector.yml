name: Metrics Collector

on:
  workflow_run:
    workflows:
      - "HQ Smoke Tests"
      - "Python Request API"
      - "CO BHA Smoke Tests"
      - "Lookup table Tests"
      - "Case Search Tests"
      - "Case Search Split Screen Tests"
      - "Data Dictionary Tests"
      - "Elastic Search Tests"
      - "Export Tests"
      - "Find Data by ID Tests"
      - "Formplayer Tests"
      - "Multi Select Tests"
      - "Priority Escape Defects Tests"
      - "Power BI Tests"
    types: [completed]
    branches:
      - master


  workflow_dispatch:
    inputs:
      latest_runs:
        description: "Ingest latest N completed runs"
        required: false
        default: "10"

  schedule:
    - cron: "30 18 * * *"   # 12:00 AM IST

permissions:
  contents: write
  actions: read
  pages: write
  id-token: write


concurrency:
  group: metrics-collector
  cancel-in-progress: false

jobs:
  collect:
    if: >
      github.event_name != 'workflow_run' ||
      github.event.workflow_run.conclusion != 'skipped'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout metrics branch
        uses: actions/checkout@v4
        with:
          ref: metrics
          fetch-depth: 0

      - name: Resolve run IDs to ingest
        id: runs
        uses: actions/github-script@v7
        with:
          script: |
            const owner = context.repo.owner;
            const repo = context.repo.repo;

            if (context.eventName === "workflow_run") {
              const run = context.payload.workflow_run;
              if (run.conclusion !== "success" && run.conclusion !== "failure") {
                core.info("Skipping non-completed run");
                core.setOutput("ids", JSON.stringify([]));
                return;
              }
              core.setOutput("ids", JSON.stringify([run.id]));
              return;
            }


            const limit = Number(core.getInput("latest_runs") || "10");
            const res = await github.rest.actions.listWorkflowRunsForRepo({
              owner,
              repo,
              status: "completed",
              per_page: limit,
            });

            core.setOutput("ids", JSON.stringify(res.data.workflow_runs.map(r => r.id)));

      - name: Download run-summary artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require("fs");
            const ids = JSON.parse(`${{ steps.runs.outputs.ids }}`);
            fs.mkdirSync("incoming", { recursive: true });

            for (const runId of ids) {
              const arts = await github.rest.actions.listWorkflowRunArtifacts({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: runId,
              });

              for (const a of arts.data.artifacts.filter(x => x.name.startsWith("run-summary"))) {
                const zip = await github.rest.actions.downloadArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: a.id,
                  archive_format: "zip",
                });
                fs.writeFileSync(`incoming/${runId}__${a.name}.zip`, Buffer.from(zip.data));
              }
            }

      - name: Unzip artifacts (safe)
        run: |
          shopt -s nullglob
          found=false
          for z in incoming/*.zip; do
            found=true
            d="${z%.zip}"
            mkdir -p "$d"
            unzip -o "$z" -d "$d" >/dev/null
          done
          if [ "$found" = false ]; then
            echo "No artifacts to unzip"
          fi
      

      - name: Append to metrics dataset (deduped)
        run: |
          set -e
          mkdir -p metrics
          touch metrics/runs.jsonl

          python <<'PY'
          import json, os
          
          existing_ids = set()
          if os.path.exists("metrics/runs.jsonl"):
              with open("metrics/runs.jsonl") as f:
                  for line in f:
                      try:
                          obj = json.loads(line)
                          rid = (
                              obj.get("github", {}).get("run_id")
                              or obj.get("run_id")
                          )
                          if rid:
                              existing_ids.add(str(rid))
                      except Exception:
                          pass
          
          added = 0
          with open("metrics/runs.jsonl", "a", encoding="utf-8") as out:
              for root, _, files in os.walk("incoming"):
                  for name in files:
                      if name == "run_summary.json":
                          path = os.path.join(root, name)
                          with open(path, encoding="utf-8") as fh:
                              obj = json.load(fh)
          
                          rid = (
                              obj.get("github", {}).get("run_id")
                              or obj.get("run_id")
                          )
          
                          if not rid:
                              continue
          
                          rid = str(rid)
                          if rid in existing_ids:
                              continue
          
                          out.write(json.dumps(obj, ensure_ascii=False) + "\n")
                          existing_ids.add(rid)
                          added += 1
          
          print(f"Added {added} new runs")

          PY

      - name: Commit metrics
        run: |
          git config user.name "metrics-bot"
          git config user.email "metrics-bot@users.noreply.github.com"
          git add metrics/runs.jsonl
          git commit -m "Metrics update ${GITHUB_RUN_ID}" || echo "No new metrics"
          git push origin metrics

  publish_dashboard:
    needs: collect
    runs-on: ubuntu-latest
    environment:
      name: github-pages

    steps:
      - uses: actions/checkout@v4
        with:
          ref: metrics

      - run: |
          rm -rf site
          mkdir -p site/data
          cp dashboard/index.html site/index.html
          cp metrics/runs.jsonl site/data/runs.jsonl

      - uses: actions/upload-pages-artifact@v3
        with:
          path: site

      - uses: actions/deploy-pages@v4

daily_digest:
  needs: publish_dashboard
  runs-on: ubuntu-latest

  steps:
    - uses: actions/checkout@v4
      with:
        ref: metrics

    - name: Build Slack summary
      run: |
        python - <<'PY'
import json, datetime
from collections import defaultdict

now = datetime.datetime.now(datetime.timezone.utc)
cutoff = now - datetime.timedelta(hours=24)

stats = defaultdict(lambda: {"pass": 0, "fail": 0})
total = 0

with open("metrics/runs.jsonl", encoding="utf-8") as f:
    for line in f:
        r = json.loads(line)
        ts = r.get("timestamp_utc")
        if not ts:
            continue

        t = datetime.datetime.fromisoformat(ts.replace("Z", "+00:00"))
        if t < cutoff:
            continue

        env = r.get("env") or r.get("environment") or "unknown"
        total += 1

        if (r.get("status") or "").lower() == "pass":
            stats[env]["pass"] += 1
        else:
            stats[env]["fail"] += 1

lines = [f"â€¢ *{e}* â†’ âœ… {v['pass']} | âŒ {v['fail']}" for e, v in stats.items()]
fail_total = sum(v["fail"] for v in stats.values())
fail_rate = round((fail_total / total * 100), 1) if total else 0

message = (
    "ðŸ“Š *QA Automation Dashboard (last 24h)*\n\n"
    "*Environment summary:*\n"
    + ("\n".join(lines) if lines else "_No runs in last 24h_")
    + f"\n\nâ€¢ Runs: *{total}*\n"
    + f"â€¢ Fail rate: *{fail_rate}%*\n\n"
    + "ðŸ”— https://dimagi.github.io/dimagi-qa/"
)

with open("slack_message.txt", "w", encoding="utf-8") as f:
    f.write(message)
PY

    - name: Post Slack update
      run: |
        curl -s -X POST https://slack.com/api/chat.postMessage \
          -H "Authorization: Bearer ${{ secrets.SLACK_QA_BOT_TOKEN }}" \
          -H "Content-Type: application/json" \
          --data "$(jq -n \
            --arg channel "${{ secrets.SLACK_CHANNEL_ID_DASHBOARD }}" \
            --arg text "$(cat slack_message.txt)" \
            '{channel:$channel,text:$text}')"
